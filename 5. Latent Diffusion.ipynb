{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c92e66",
   "metadata": {},
   "source": [
    "# Latent Diffusion\n",
    "\n",
    "In the previous notebooks, we've seen how diffusion models can generate images by learning to denoise pixel space directly. However, working directly in pixel space has limitations: high-resolution images require enormous computational resources, and the network must learn to model both high-level structure (like \"this is a digit\") and low-level details (like individual pixel values) simultaneously.\n",
    "\n",
    "**Latent diffusion** solves this by working in a compressed **latent space** instead of pixel space. The key idea is:\n",
    "\n",
    "1. **Encode**: Use a Variational Autoencoder (VAE) to compress images into a smaller latent representation (e.g., 32×32 pixels → 4×4 latents)\n",
    "2. **Diffuse**: Train the diffusion model to work in this compressed latent space\n",
    "3. **Decode**: Use the VAE decoder to convert the generated latents back to pixel space\n",
    "\n",
    "This approach offers several advantages:\n",
    "- **Computational efficiency**: The diffusion model processes much smaller tensors (4×4 instead of 32×32), making training and inference faster\n",
    "- **Better representation**: The latent space captures semantic information more compactly, allowing the model to focus on high-level structure\n",
    "- **Scalability**: This is the approach used by Stable Diffusion and other state-of-the-art models to generate high-resolution images efficiently\n",
    "\n",
    "In this notebook, we'll:\n",
    "- Build and train a VAE to learn a compressed representation of MNIST digits\n",
    "- Train a diffusion model (using Flow Matching) in the latent space\n",
    "- Generate new images by sampling in latent space and decoding to pixels\n",
    "Reference paper: [High-Resolution Image Synthesis with Latent Diffusion Models\n",
    "](https://arxiv.org/abs/2112.10752)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45178846",
   "metadata": {},
   "source": [
    "### Step 1: Building the Variational Autoencoder (VAE)\n",
    "\n",
    "A Variational Autoencoder consists of two main components:\n",
    "\n",
    "- **Encoder**: Compresses input images into a latent representation. Instead of producing a single latent code, it outputs a **mean** ($\\mu$) and **log-variance** ($\\log \\sigma^2$) for each latent dimension, allowing us to sample from a distribution rather than a fixed point.\n",
    "\n",
    "- **Decoder**: Reconstructs images from latent codes. It learns to map the compressed representation back to pixel space.\n",
    "\n",
    "The key feature of VAEs is the **reparameterization trick**: instead of sampling directly from the encoder's output distribution (which would break backpropagation), we sample from a standard Gaussian and transform it using the learned mean and variance:\n",
    "\n",
    "$$z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$$\n",
    "\n",
    "This makes the sampling process differentiable, allowing end-to-end training.\n",
    "\n",
    "Our VAE architecture:\n",
    "- **Encoder**: Three convolutional layers that progressively downsample 32×32 images to 4×4 feature maps\n",
    "- **Latent space**: 4 channels at 4×4 resolution (64× compression from 32×32×1)\n",
    "- **Decoder**: Three transposed convolutional layers that upsample back to 32×32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68be19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=1, latent_dim=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: 32x32 -> 4x4\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 32 -> 16\n",
    "            nn.Conv2d(input_dim, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            # 16 -> 8\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            # 8 -> 4\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        # To Mean and Variance\n",
    "        # We need to predict a Mean map AND a Log-Variance map.\n",
    "        self.to_moments = nn.Conv2d(64, 2 * latent_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        # Decoder (Mirror image)\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 4 -> 8\n",
    "            nn.ConvTranspose2d(latent_dim, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            # 8 -> 16\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            # 16 -> 32\n",
    "            nn.ConvTranspose2d(32, input_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh() # Map to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Reparametrization trick\n",
    "        epsilon = torch.randn_like(mu)\n",
    "        z = mu + epsilon * torch.exp(0.5 * logvar) # z = mu + sigma * epsilon\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        enc = self.encoder(x)\n",
    "        moments = self.to_moments(enc)\n",
    "        mu, logvar = moments.chunk(2, dim=1)\n",
    "        \n",
    "        # Sample\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Decode\n",
    "        recon = self.decoder(z)\n",
    "        return recon, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c5734",
   "metadata": {},
   "source": [
    "### The VAE Loss Function\n",
    "\n",
    "Training a VAE requires balancing two objectives:\n",
    "\n",
    "1. **Reconstruction Loss**: The decoded image should match the original input. We use Mean Squared Error (MSE) to measure how well the VAE reconstructs the input.\n",
    "\n",
    "2. **KL Divergence Loss**: The learned latent distribution should be close to a standard Gaussian $\\mathcal{N}(0, 1)$. This regularization encourages the latent space to be smooth and well-structured, making it easier for the diffusion model to learn meaningful patterns later.\n",
    "\n",
    "The total loss is:\n",
    "$$\\mathcal{L}_{\\text{VAE}} = \\mathcal{L}_{\\text{recon}} + \\beta \\cdot \\mathcal{L}_{\\text{KL}}$$\n",
    "\n",
    "Where $\\mathcal{L}_{\\text{KL}} = -0.5 \\sum (1 + \\log \\sigma^2 - \\mu^2 - \\sigma^2)$ encourages the latent distribution to match a standard Gaussian. In our implementation, we use $\\beta = 1$ (though some variants like $\\beta$-VAE use different values to control the trade-off).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    # 1. Reconstruction Loss (MSE)\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # 2. KL Divergence\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - torch.pow(mu, 2) - torch.exp(logvar))\n",
    "    \n",
    "    return recon_loss + kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f36ae",
   "metadata": {},
   "source": [
    "### Step 2: Training the VAE\n",
    "\n",
    "Now we'll train the VAE to learn a good compressed representation of our MNIST digits. The training process is straightforward: for each image, we encode it to a latent, sample from the latent distribution, decode back to pixels, and minimize the combined reconstruction and KL divergence loss.\n",
    "\n",
    "Once trained, the VAE encoder will compress each 32×32 image into a 4×4×4 latent representation, and the decoder will be able to reconstruct reasonable approximations of the original images from these latents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38790cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(image_size=32, batch_size=128, device='cpu'):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "    ])\n",
    "    dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    \n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28730476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset, dataloader = load_mnist(image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0533951",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_EPOCHS = 20\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "vae_model = VAE().to(device)\n",
    "vae_optimizer = optim.Adam(vae_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. VAE Training Loop ---\n",
    "print(\"Starting VAE Training...\")\n",
    "vae_model.train()\n",
    "\n",
    "for epoch in range(VAE_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for step, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_images, mu, logvar = vae_model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = vae_loss(recon_images, images, mu, logvar)\n",
    "        \n",
    "        # Backpropagation\n",
    "        vae_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        vae_optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "    print(f\"Epoch {epoch} Avg Loss: {sum(epoch_losses)/len(epoch_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2c11c",
   "metadata": {},
   "source": [
    "Let's visualize some reconstructions to see how well our VAE learned to compress and decompress the images. The top row shows original images, and the bottom row shows their reconstructions from the compressed latent representation. Some blurriness is expected—this is the trade-off for compression, but the VAE should preserve the essential structure of each digit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b73597",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model.eval()\n",
    "\n",
    "# 1. Get a batch\n",
    "images, _ = next(iter(dataloader))\n",
    "images = images.to(device)\n",
    "\n",
    "# 2. Reconstruct\n",
    "with torch.no_grad():\n",
    "    recon_images, _, _ = vae_model(images)\n",
    "\n",
    "# 3. Plot\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 4))\n",
    "\n",
    "# Top row: Originals\n",
    "for i in range(8):\n",
    "    # Un-normalize\n",
    "    img = (images[i, 0].cpu() + 1) / 2\n",
    "    axes[0, i].imshow(img, cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    if i == 0: axes[0, i].set_title(\"Original\")\n",
    "\n",
    "# Bottom row: Reconstructions\n",
    "for i in range(8):\n",
    "    # Un-normalize\n",
    "    img = (recon_images[i, 0].cpu() + 1) / 2\n",
    "    axes[1, i].imshow(img, cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0: axes[1, i].set_title(\"Reconstructed\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7be4fe",
   "metadata": {},
   "source": [
    "Once we're done with training our vae model, it's time for latent diffusion. First, we create and encoded version of our dataset, such that we can skip encoding each time every image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_dataset(model, dataloader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"Encoding dataset...\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Pass images through the encoder\n",
    "            enc = model.encoder(images) \n",
    "            # Get the mean (mu)\n",
    "            moments = model.to_moments(enc)\n",
    "            mu, _ = moments.chunk(2, dim=1)\n",
    "            \n",
    "            latents.append(mu.cpu())\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "    return torch.cat(latents, dim=0), torch.cat(all_labels, dim=0)\n",
    "\n",
    "latent_data, latent_labels = encode_dataset(vae_model, dataloader)\n",
    "print(f'latent_data shape: {latent_data.shape}')  # Should be (N, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6899d",
   "metadata": {},
   "source": [
    "### Step 3: The Latent Diffusion Model\n",
    "\n",
    "Now that we have a compressed latent representation, we can train a diffusion model to work in this space. Since we're working with 4×4 latents instead of 32×32 images, we can use a simpler architecture than the full U-Net we saw before.\n",
    "\n",
    "Our **LatentDiffuser** is essentially a ResNet-style network:\n",
    "- **Input projection**: Maps the 4-channel latent to a higher-dimensional feature space (64 channels)\n",
    "- **Processing blocks**: A sequence of residual blocks that process the latent while maintaining the 4×4 spatial size\n",
    "- **Output projection**: Maps back to 4 channels (matching the latent dimension)\n",
    "\n",
    "Each block incorporates:\n",
    "- **Time embeddings**: So the network knows which timestep it's denoising\n",
    "- **Label embeddings**: For conditional generation (we'll condition on digit labels, 0-9)\n",
    "\n",
    "Unlike the U-Net, we don't need downsampling/upsampling because we're already working at the compressed resolution. This makes the architecture simpler and faster to train.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703ce05",
   "metadata": {},
   "source": [
    "We'll reuse the `SinusoidalEmbedding` class from previous notebooks to encode timesteps. This allows the network to understand how much noise is present at different stages of the diffusion process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, t):\n",
    "            device = t.device\n",
    "            emb = torch.zeros(t.shape[0], self.embedding_dim, device=device)\n",
    "            \n",
    "            for i in range(self.embedding_dim // 2):\n",
    "                # We ensure the constant is on the correct device too\n",
    "                const = torch.tensor(10000.0, device=device) \n",
    "                omega_i = torch.exp(-(2*i/self.embedding_dim) * torch.log(const))\n",
    "                \n",
    "                emb[:, 2*i] = torch.sin(omega_i * t)\n",
    "                emb[:, 2*i+1] = torch.cos(omega_i * t)\n",
    "            \n",
    "            return emb\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. First Convolution: Change channels (e.g. 1 -> 32)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 2. Time Projection: Map time to match 'out_ch'\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        \n",
    "        # 3. Second Convolution: Refine features\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # First Conv\n",
    "        h = self.conv1(x)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu(h)\n",
    "        \n",
    "        # Add Time Embedding\n",
    "        # (Batch, Time_Dim) -> (Batch, Out_Ch) -> (Batch, Out_Ch, 1, 1)\n",
    "        time_emb = self.time_mlp(t)\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2] # Broadcast to 4D\n",
    "        h = h + time_emb\n",
    "        \n",
    "        # Second Conv\n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.relu(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cd1bf",
   "metadata": {},
   "source": [
    "As we already reached a compact representation via the variational autoencoder, we should now change the UNet structure such that we skip the contraption part. We define a new latent diffuser model, which is basically a ResNet based on our Block modules working in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffuser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Time Embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalEmbedding(32),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 1. Input Projection (4 channels -> 64 channels)\n",
    "        self.conv_in = nn.Conv2d(4, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 2. Body (Just a sequence of Blocks, keeping size 4x4)\n",
    "        self.block1 = Block(64, 128, 32)\n",
    "        self.block2 = Block(128, 256, 32)\n",
    "        self.block3 = Block(256, 128, 32) # Shrink channels back down\n",
    "        \n",
    "        # 3. Output Projection (128 channels -> 4 channels)\n",
    "        self.final = nn.Conv2d(128, 4, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Label Embedding (for Conditional Generation)\n",
    "        self.label_emb = nn.Embedding(10, 32)\n",
    "\n",
    "    def forward(self, x, t, labels):\n",
    "        # Embed Time & Labels\n",
    "        t = self.time_mlp(t)\n",
    "        label_emb = self.label_emb(labels)\n",
    "        \n",
    "        t = t + label_emb\n",
    "        \n",
    "        # Run the blocks\n",
    "        x = self.conv_in(x)\n",
    "        \n",
    "        x = self.block1(x, t)\n",
    "        x = self.block2(x, t)\n",
    "        x = self.block3(x, t)\n",
    "        \n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58304bd2",
   "metadata": {},
   "source": [
    "We'll create a simple dataset wrapper to work with our pre-encoded latents. This allows us to use PyTorch's DataLoader for efficient batching during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, latent_data, labels):\n",
    "        self.latent_data = latent_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.latent_data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.latent_data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dataset = TensorDataset(latent_data, latent_labels)\n",
    "dataloader = DataLoader(latent_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8fbd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuser = LatentDiffuser().to(device)\n",
    "optimizer = optim.Adam(diffuser.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa92e7",
   "metadata": {},
   "source": [
    "### Flow Matching in Latent Space\n",
    "We train our network based on a standarf Flow Matching approach, with the difference that now the network will learn to reconstruct the latent representations of images, instead of reconstructing the raw pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flow_match_loss_conditional(model, x_1, labels):\n",
    "    # x_1: Real data batch (Batch, C, H, W)\n",
    "    b = x_1.shape[0]\n",
    "    \n",
    "    # 1. Sample x_0 (Pure Gaussian Noise)\n",
    "    x_0 = torch.randn_like(x_1)\n",
    "    \n",
    "    # 2. Sample random time t (uniform 0 to 1)\n",
    "    # Make sure to reshape it to (B, 1, 1, 1) for broadcasting!\n",
    "    t = torch.rand((b,), device=x_1.device)\n",
    "    t_broad = t.view(-1, 1, 1, 1)\n",
    "    \n",
    "    # 3. Compute x_t (Linear Interpolation)\n",
    "    x_t = (1-t_broad) * x_0 + t_broad * x_1\n",
    "    \n",
    "    # 4. Compute Target Velocity (The formula you derived)\n",
    "    target_v = x_1 - x_0\n",
    "    \n",
    "    # 5. Predict with model\n",
    "    # Note: We need to pass 't' (vector) not 't_broad' to the model usually, \n",
    "    # depending on your embedding layer. Let's assume 't' is fine.\n",
    "    pred_v = model(x_t, t, labels)\n",
    "    \n",
    "    # 6. MSE Loss\n",
    "    return torch.nn.functional.mse_loss(pred_v, target_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_EPOCHS = 20\n",
    "\n",
    "print(\"Starting Latent Flow Matching Training...\")\n",
    "diffuser.train()\n",
    "\n",
    "for epoch in range(LATENT_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    for step, (latents, labels) in enumerate(dataloader):\n",
    "        latents = latents.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Calculate Flow Matching Loss\n",
    "        loss = compute_flow_match_loss_conditional(diffuser, latents, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    print(f\"Epoch {epoch} Avg Loss: {sum(epoch_losses)/len(epoch_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f35e99",
   "metadata": {},
   "source": [
    "### Step 4: Generating Images from Latent Space\n",
    "\n",
    "Now we can generate new images! The process has three stages:\n",
    "\n",
    "1. **Sample in latent space**: Start with pure Gaussian noise in the latent space and use the trained diffuser to iteratively denoise it using Flow Matching. We use Euler's method to integrate the velocity field over multiple steps.\n",
    "\n",
    "2. **Decode to pixel space**: Once we have a clean latent, pass it through the VAE decoder to get a 32×32 image.\n",
    "\n",
    "3. **Visualize**: Display the generated images.\n",
    "\n",
    "The beauty of this approach is that all the expensive diffusion computation happens in the small 4×4 latent space, and we only decode once at the end. This makes generation much faster than working directly in pixel space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_latent_flow_matching(diffuser, vae, labels, steps=50, device=device):\n",
    "    diffuser.eval()\n",
    "    vae.eval()\n",
    "    n_samples = len(labels)\n",
    "    \n",
    "    # 1. Start with LATENT noise (4x4 spatial size, 4 channels)\n",
    "    # Note: size is 4, not 32!\n",
    "    x = torch.randn(n_samples, 4, 4, 4, device=device)\n",
    "    \n",
    "    dt = 1.0 / steps\n",
    "    \n",
    "    # 2. Run Flow Matching (Latent Space)\n",
    "    for i in range(steps):\n",
    "        t = i * dt\n",
    "        t_batch = torch.full((n_samples,), t, device=device)\n",
    "        \n",
    "        # Get velocity\n",
    "        v = diffuser(x, t_batch, labels)\n",
    "        \n",
    "        # Euler step\n",
    "        x = x + v * dt\n",
    "        \n",
    "    # 3. Decode (Latent -> Pixel)\n",
    "    # x is now our generated latent. Pass it to VAE decoder.\n",
    "    print(\"Decoding latents...\")\n",
    "    decoded_images = vae.decoder(x)\n",
    "    \n",
    "    return decoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Run Generation ---\n",
    "labels = torch.arange(10).to(device)\n",
    "# Combine to get 20 samples (0-9, 0-9)\n",
    "labels = torch.cat([labels, labels]) \n",
    "\n",
    "final_images = sample_latent_flow_matching(diffuser, vae_model, labels, steps=5)\n",
    "\n",
    "# --- Plotting ---\n",
    "final_images = (final_images + 1) / 2\n",
    "final_images = final_images.clamp(0, 1).cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 10, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(final_images[i, 0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"{labels[i].item()}\")\n",
    "plt.suptitle(\"Latent Flow Matching Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1edef",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "We've now implemented a complete latent diffusion pipeline! This approach combines the best of both worlds: the expressive power of diffusion models with the efficiency of working in compressed latent space.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **VAE Compression**: Using a Variational Autoencoder to learn a compact latent representation of images\n",
    "- **Latent Space Diffusion**: Training the diffusion model to work in the compressed space rather than pixel space\n",
    "- **Flow Matching**: An alternative to DDPM that learns velocity fields for faster and more stable training\n",
    "- **End-to-End Pipeline**: Encoding → Diffusion → Decoding to generate high-quality images efficiently\n",
    "\n",
    "**Why This Matters:**\n",
    "Latent diffusion is the foundation of modern image generation models like Stable Diffusion. By working in latent space, we can:\n",
    "- Generate high-resolution images with manageable computational costs\n",
    "- Focus the model's capacity on learning semantic structure rather than pixel-level details\n",
    "- Scale to larger images and more complex datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
