{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ddf26a0",
   "metadata": {},
   "source": [
    "## Generating images\n",
    "\n",
    "In the previous notebook, we learned the fundamentals of diffusion models using a simple 2D spiral dataset. We saw how a neural network can learn to reverse the forward diffusion process, gradually denoising random noise back into structured data.\n",
    "\n",
    "Now, let's take the next step: applying diffusion models to **real images**. The core principles remain the same, but we need to adapt our approach to handle the complexity of image data. Instead of simple 2D points, we'll work with pixel values arranged in a grid, and we'll need a more sophisticated architecture—the U-Net—to effectively process spatial information.\n",
    "\n",
    "In this notebook, we'll:\n",
    "- Apply diffusion to the MNIST dataset (handwritten digits)\n",
    "- Build a U-Net architecture designed for image denoising\n",
    "- Train a model to generate realistic digit images\n",
    "- Extend to conditional generation, where we can specify which digit to generate\n",
    "\n",
    "The journey from 2D points to images is exciting because it demonstrates how the same mathematical framework scales to much more complex data distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fa052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5429cc",
   "metadata": {},
   "source": [
    "### Reusing Core Components\n",
    "\n",
    "Before we dive into images, let's recall the essential building blocks from our previous notebook. We'll reuse the forward diffusion sampling function and the sinusoidal time embedding, which are fundamental to any diffusion model regardless of the data type.\n",
    "\n",
    "The key insight is that the forward diffusion process works the same way for images as it did for 2D points—we're just applying it to a higher-dimensional tensor (height × width × channels) instead of a simple 2D vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73284e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_t(x_0, t, alpha_bars):\n",
    "    epsilon = torch.randn_like(x_0)\n",
    "    x_t = torch.sqrt(alpha_bars[t]) * x_0 + torch.sqrt(1-alpha_bars[t]) * epsilon\n",
    "    \n",
    "    return x_t, epsilon\n",
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, t):\n",
    "            device = t.device\n",
    "            emb = torch.zeros(t.shape[0], self.embedding_dim, device=device)\n",
    "            \n",
    "            for i in range(self.embedding_dim // 2):\n",
    "                # We ensure the constant is on the correct device too\n",
    "                const = torch.tensor(10000.0, device=device) \n",
    "                omega_i = torch.exp(-(2*i/self.embedding_dim) * torch.log(const))\n",
    "                \n",
    "                emb[:, 2*i] = torch.sin(omega_i * t)\n",
    "                emb[:, 2*i+1] = torch.cos(omega_i * t)\n",
    "            \n",
    "            return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2444f34",
   "metadata": {},
   "source": [
    "### Step 1: Preparing Image Data\n",
    "\n",
    "For our image generation experiment, we'll use the MNIST dataset, a classic benchmark consisting of 28×28 grayscale images of handwritten digits (0-9). This will allow us to work with a model that requires minimal compute, but still works on the same basics of well established image generation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b86d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 2. Load MNIST Data\n",
    "# We add padding to make it 32x32 (standard power of 2 is easier for U-Nets)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1) # Scale from [0, 1] to [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cfaaf",
   "metadata": {},
   "source": [
    "Now let's see the forward diffusion process in action! We'll take a clean digit image and observe how it becomes progressively noisier at different timesteps. This visualization confirms that our noise schedule works correctly for images, just as it did for 2D points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1cc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualize the Forward Process on Images\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2), # Scale back to [0, 1]\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC for plot\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "    \n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :] \n",
    "    plt.imshow(reverse_transforms(image), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cad6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify the noise schedule on a digit\n",
    "# Reuse your existing 'sample_t' function!\n",
    "images, labels = next(iter(dataloader))\n",
    "x_0 = images[0:1] # Take the first image\n",
    "\n",
    "# Timesteps to visualize\n",
    "steps = [0, 50, 100, 150, 199]\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# We need your alphas/betas definitions again for 200 steps\n",
    "betas = torch.linspace(1e-4, 0.02, 200)\n",
    "alphas = 1 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "for idx, t in enumerate(steps):\n",
    "    plt.subplot(1, len(steps), idx + 1)\n",
    "    # Note: We need to pass t as a tensor\n",
    "    t_tensor = torch.tensor([t])\n",
    "    \n",
    "    # Generate noisy image\n",
    "    x_t, _ = sample_t(x_0, t_tensor, alpha_bars)\n",
    "    \n",
    "    show_tensor_image(x_t)\n",
    "    plt.title(f\"Step {t}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11707081",
   "metadata": {},
   "source": [
    "### Step 2: The U-Net Architecture\n",
    "\n",
    "When we worked with 2D points, a simple fully-connected network was sufficient. But images have a crucial property: **spatial structure**. Nearby pixels are more related than distant ones, and we want our network to understand these local relationships.\n",
    "\n",
    "This is where the **U-Net** architecture shines. Originally designed for medical image segmentation, U-Nets are perfect for diffusion models because they:\n",
    "\n",
    "1. **Preserve spatial information**: Unlike fully-connected layers that flatten everything, convolutions maintain the 2D structure\n",
    "2. **Use skip connections**: The U-shaped design allows the network to combine high-level features (like \"this is a digit\") with low-level details (like \"this edge is sharp\")\n",
    "3. **Handle multi-scale features**: The encoder-decoder structure lets the network understand both global shape and fine details\n",
    "\n",
    "The architecture consists of:\n",
    "- **Down path (Encoder)**: Progressively downsamples the image, extracting increasingly abstract features\n",
    "- **Bottleneck**: The deepest layer where the most compressed representation lives\n",
    "- **Up path (Decoder)**: Progressively upsamples back to the original resolution, using skip connections to recover fine details\n",
    "\n",
    "Each block in our U-Net will:\n",
    "- Apply convolutions to process spatial information\n",
    "- Incorporate time embeddings so the network knows which timestep it's denoising\n",
    "- Use batch normalization for stable training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0c6cc",
   "metadata": {},
   "source": [
    "First, we define a basic building block that will be used throughout the U-Net. Each block performs two convolutions with batch normalization and ReLU activations. Crucially, it also incorporates the time embedding, allowing the network to adapt its behavior based on how much noise is present at the current timestep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. First Convolution: Change channels (e.g. 1 -> 32)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 2. Time Projection: Map time to match 'out_ch'\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        \n",
    "        # 3. Second Convolution: Refine features\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # First Conv\n",
    "        h = self.conv1(x)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu(h)\n",
    "        \n",
    "        # Add Time Embedding\n",
    "        # (Batch, Time_Dim) -> (Batch, Out_Ch) -> (Batch, Out_Ch, 1, 1)\n",
    "        time_emb = self.time_mlp(t)\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2] # Broadcast to 4D\n",
    "        h = h + time_emb\n",
    "        \n",
    "        # Second Conv\n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.relu(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1425c0",
   "metadata": {},
   "source": [
    "Now we assemble the full U-Net. The architecture follows a symmetric pattern:\n",
    "\n",
    "- **Down path**: Three blocks that progressively reduce spatial dimensions (32×32 → 16×16 → 8×8 → 4×4) while increasing channels (1 → 32 → 64 → 128)\n",
    "- **Bottleneck**: A single block at the deepest level (256 channels, 4×4 resolution)\n",
    "- **Up path**: Three blocks that progressively increase spatial dimensions back to 32×32, using skip connections to combine with features from the down path\n",
    "\n",
    "The skip connections (concatenating features from the down path) are crucial, as they allow the network to preserve fine-grained details that would otherwise be lost during downsampling. This is why the architecture looks like a \"U\"—information flows down, then back up, with shortcuts connecting corresponding levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324342fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Time Embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalEmbedding(32),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 1. Down Path\n",
    "        self.down1 = Block(1, 32, 32)\n",
    "        self.down2 = Block(32, 64, 32)\n",
    "        self.down3 = Block(64, 128, 32)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 2. Bottleneck\n",
    "        self.bottleneck = Block(128, 256, 32)\n",
    "        \n",
    "        # 3. Up Path\n",
    "        # We separate the Upsampling (ConvTrans) from the Block (Processing)\n",
    "        \n",
    "        # Up 1: 4x4 -> 8x8\n",
    "        self.up_trans1 = nn.ConvTranspose2d(256, 256, 4, 2, 1) \n",
    "        self.up1 = Block(256 + 128, 128, 32) # In: Bottle + Skip(x3)\n",
    "        \n",
    "        # Up 2: 8x8 -> 16x16\n",
    "        self.up_trans2 = nn.ConvTranspose2d(128, 128, 4, 2, 1)\n",
    "        self.up2 = Block(128 + 64, 64, 32)   # In: Prev + Skip(x2)\n",
    "        \n",
    "        # Up 3: 16x16 -> 32x32\n",
    "        self.up_trans3 = nn.ConvTranspose2d(64, 64, 4, 2, 1)\n",
    "        self.up3 = Block(64 + 32, 32, 32)    # In: Prev + Skip(x1)\n",
    "        \n",
    "        # Final projection\n",
    "        self.final = nn.Conv2d(32, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = self.time_mlp(t)\n",
    "        \n",
    "        # --- Down Path ---\n",
    "        x1 = self.down1(x, t)        # (32, 32, 32)\n",
    "        x_p1 = self.pool(x1)         # (32, 16, 16)\n",
    "        \n",
    "        x2 = self.down2(x_p1, t)     # (64, 16, 16)\n",
    "        x_p2 = self.pool(x2)         # (64, 8, 8)\n",
    "        \n",
    "        x3 = self.down3(x_p2, t)     # (128, 8, 8)\n",
    "        x_p3 = self.pool(x3)         # (128, 4, 4)\n",
    "        \n",
    "        # --- Bottleneck ---\n",
    "        x = self.bottleneck(x_p3, t) # (256, 4, 4)\n",
    "        \n",
    "        # --- Up Path ---\n",
    "        \n",
    "        # Step 1: Upsample -> Concat -> Process\n",
    "        x = self.up_trans1(x)                          # (256, 8, 8)\n",
    "        x = self.up1(torch.cat((x, x3), dim=1), t)     # (128, 8, 8)\n",
    "        \n",
    "        # Step 2\n",
    "        x = self.up_trans2(x)                          # (128, 16, 16)\n",
    "        x = self.up2(torch.cat((x, x2), dim=1), t)     # (64, 16, 16)\n",
    "        \n",
    "        # Step 3\n",
    "        x = self.up_trans3(x)                          # (64, 32, 32)\n",
    "        x = self.up3(torch.cat((x, x1), dim=1), t)     # (32, 32, 32)\n",
    "        \n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e705cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(image_size=32, batch_size=128, device='cpu'):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "    ])\n",
    "    dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    \n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a3806",
   "metadata": {},
   "source": [
    "### Step 3: Training Setup\n",
    "\n",
    "Now that we have our architecture, let's set up the training pipeline. The training process for images follows the same principle as our 2D spiral example:\n",
    "\n",
    "1. Sample a random timestep $t$ for each image in the batch\n",
    "2. Add noise to the clean image using the forward diffusion shortcut: $x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$\n",
    "3. Ask the network to predict the noise $\\epsilon$ given $x_t$ and $t$\n",
    "4. Minimize the difference between predicted and actual noise\n",
    "\n",
    "The key difference is that now our network processes 2D images instead of 1D vectors, and we use a U-Net instead of a fully-connected network.\n",
    "\n",
    "Let's prepare our data loader and model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "dataset, dataloader = load_mnist(image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 300\n",
    "EPOCHS = 5 \n",
    "\n",
    "# --- 3. Model & Utils ---\n",
    "model = SimpleUNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Recalculate alphas/betas for the image process\n",
    "betas = torch.linspace(1e-4, 0.02, TIMESTEPS).to(device)\n",
    "alphas = 1 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff999bf",
   "metadata": {},
   "source": [
    "### Step 4: Training the Model\n",
    "\n",
    "The training loop is straightforward but powerful. For each batch of images, we:\n",
    "1. Randomly sample timesteps (different noise levels for each image)\n",
    "2. Add the corresponding amount of noise\n",
    "3. Have the U-Net predict what noise was added\n",
    "4. Update the network to minimize this prediction error\n",
    "\n",
    "By training on many different noise levels simultaneously, the network learns to handle the entire denoising trajectory, from pure noise all the way to clean images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noisy_image(x_0, t):\n",
    "    # This now handles batch of images (B, C, H, W)\n",
    "    sqrt_alpha_bar = torch.sqrt(alpha_bars[t])[:, None, None, None]\n",
    "    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n",
    "    \n",
    "    epsilon = torch.randn_like(x_0)\n",
    "    x_t = sqrt_alpha_bar * x_0 + sqrt_one_minus_alpha_bar * epsilon\n",
    "    return x_t, epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc853708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Training Loop ---\n",
    "print(\"Starting Training...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_losses = []\n",
    "    for step, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # 1. Sample timesteps\n",
    "        t = torch.randint(0, TIMESTEPS, (BATCH_SIZE,), device=device).long()\n",
    "        \n",
    "        # 2. Add Noise\n",
    "        x_t, epsilon = get_noisy_image(images, t)\n",
    "        \n",
    "        # 3. Predict Noise\n",
    "        pred_epsilon = model(x_t, t)\n",
    "        \n",
    "        # 4. Optimize\n",
    "        loss = loss_fn(pred_epsilon, epsilon)\n",
    "        epoch_losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "    print(f\"Average Epoch {epoch} Loss: {sum(epoch_losses) / len(epoch_losses):.4f}\")\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3286b56",
   "metadata": {},
   "source": [
    "### Step 5: Generating New Images\n",
    "\n",
    "Now we are ready to test our model. The sampling process is the reverse of training:\n",
    "\n",
    "1. Start with pure Gaussian noise $x_T$\n",
    "2. For each timestep from $T$ down to $0$:\n",
    "   - Have the network predict the noise $\\epsilon_\\theta(x_t, t)$\n",
    "   - Use the reverse diffusion formula to compute $x_{t-1}$:\n",
    "     $$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\cdot \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t \\cdot z$$\n",
    "   - Where $z$ is random noise (except at the final step where it's zero)\n",
    "\n",
    "By iteratively applying this process, we gradually transform random noise into a structured image that resembles the training data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38abccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_mnist(model, timesteps, n_samples=16):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Start with pure noise (Batch, 1, 32, 32)\n",
    "    x = torch.randn(n_samples, 1, IMAGE_SIZE, IMAGE_SIZE).to(device)\n",
    "    \n",
    "    # 2. Loop backwards\n",
    "    for t in range(timesteps - 1, -1, -1):\n",
    "        # Create batch of timesteps\n",
    "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        \n",
    "        # Get Model Prediction\n",
    "        predicted_noise = model(x, t_batch)\n",
    "        \n",
    "        # Get constants for this step\n",
    "        # We reshape them to (1, 1, 1, 1) so they broadcast over the images\n",
    "        alpha_t = alphas[t].view(1, 1, 1, 1)\n",
    "        alpha_bar_t = alpha_bars[t].view(1, 1, 1,| 1)\n",
    "        beta_t = betas[t].view(1, 1, 1, 1)\n",
    "        sigma_t = torch.sqrt(beta_t)\n",
    "        \n",
    "        # Algorithm: x_{t-1} = (x_t - ... * pred_noise) / sqrt(alpha)\n",
    "        if t > 0:\n",
    "            z = torch.randn_like(x)\n",
    "        else:\n",
    "            z = torch.zeros_like(x)\n",
    "            \n",
    "        x = (1 / torch.sqrt(alpha_t)) * (x - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * predicted_noise) + sigma_t * z\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the Sampling ---\n",
    "timesteps = 300\n",
    "betas = torch.linspace(1e-4, 0.02, timesteps).to(device)\n",
    "alphas = 1 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0).to(device)\n",
    "\n",
    "print(\"Sampling from the model...\")\n",
    "generated_images = sample_mnist(model, timesteps, n_samples=16)\n",
    "\n",
    "# --- Visualization ---\n",
    "# Un-normalize from [-1, 1] back to [0, 1]\n",
    "generated_images = (generated_images + 1) / 2\n",
    "generated_images = generated_images.clamp(0, 1).cpu()\n",
    "\n",
    "# Plot as a grid\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(generated_images[i, 0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Generated Digits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157a1d0",
   "metadata": {},
   "source": [
    "Let's also create an animated GIF showing step-by-step the reverse diffusion process from pure noise to clean digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d250b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def generate_mnist_gif(\n",
    "    model,\n",
    "    gif_path='mnist_generation.gif',\n",
    "    timesteps=300,\n",
    "    n_samples=16,\n",
    "    grid_size=(4, 4),\n",
    "    duration=60,\n",
    "    device=None,\n",
    "    image_size=None,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a diffusion MNIST generation gif from a trained model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained (unconditional) model.\n",
    "        gif_path: Path for saving the gif.\n",
    "        timesteps: Number of diffusion steps (default: 300).\n",
    "        n_samples: Number of images for the gif (must match grid_size product).\n",
    "        grid_size: Tuple (nrow, ncol) for gif grid.\n",
    "        duration: ms per frame in gif.\n",
    "        device: torch.device or None (default: model/device autodetect).\n",
    "        image_size: int or None (default: autodetect from model/x).\n",
    "        verbose: Print progress and file location.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # Setup device and image size\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    if image_size is None:\n",
    "        # Try to get from model or default to 32\n",
    "        image_size = 32\n",
    "\n",
    "    # Precompute diffusion schedule\n",
    "    betas = torch.linspace(1e-4, 0.02, timesteps, device=device)\n",
    "    alphas = 1 - betas\n",
    "    alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_with_history(model, steps, samples):\n",
    "        model.eval()\n",
    "        x = torch.randn(samples, 1, image_size, image_size, device=device)\n",
    "        history = []\n",
    "        for t in range(steps - 1, -1, -1):\n",
    "            history.append(x.detach().cpu().clone())\n",
    "            t_batch = torch.full((samples,), t, device=device, dtype=torch.long)\n",
    "            predicted_noise = model(x, t_batch)\n",
    "            alpha_t = alphas[t].view(1, 1, 1, 1)\n",
    "            alpha_bar_t = alpha_bars[t].view(1, 1, 1, 1)\n",
    "            beta_t = betas[t].view(1, 1, 1, 1)\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            z = torch.randn_like(x) if t > 0 else torch.zeros_like(x)\n",
    "            x = (1 / torch.sqrt(alpha_t)) * (\n",
    "                x - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * predicted_noise\n",
    "            ) + sigma_t * z\n",
    "        history.append(x.detach().cpu().clone())\n",
    "        return history\n",
    "\n",
    "    def make_grid_img(images, grid_size):\n",
    "        # images: Tensor [N, 1, H, W], normalized [-1,1] or [0,1]\n",
    "        imgs = (images + 1) / 2\n",
    "        imgs = imgs.clamp(0, 1)\n",
    "        imgs = imgs.cpu().numpy()\n",
    "        imgs = imgs.squeeze(1)  # [N, H, W]\n",
    "        nrow, ncol = grid_size\n",
    "        H, W = imgs.shape[1], imgs.shape[2]\n",
    "        grid = np.zeros((nrow * H, ncol * W))\n",
    "        for idx in range(images.shape[0]):\n",
    "            r = idx // ncol\n",
    "            c = idx % ncol\n",
    "            grid[r * H:(r + 1) * H, c * W:(c + 1) * W] = imgs[idx]\n",
    "        return (grid * 255).astype(np.uint8)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Sampling with recording...\")\n",
    "\n",
    "    history = sample_with_history(model, timesteps, n_samples)\n",
    "    frames = [\n",
    "        Image.fromarray(make_grid_img(step[:n_samples], grid_size=grid_size), mode='L')\n",
    "        for step in history\n",
    "    ]\n",
    "\n",
    "    # Save GIF\n",
    "    frames[0].save(\n",
    "        gif_path,\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        duration=duration,\n",
    "        loop=0,\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Animated GIF saved to {gif_path}\")\n",
    "\n",
    "    # Optionally display the initial and last generation grids\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Initial Noise\")\n",
    "    plt.imshow(np.array(frames[0]), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Final Output\")\n",
    "    plt.imshow(np.array(frames[-1]), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return gif_path, frames\n",
    "\n",
    "# Example usage (after training your model and defining IMAGE_SIZE/device):\n",
    "generate_mnist_gif(model,image_size=IMAGE_SIZE, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2382d07",
   "metadata": {},
   "source": [
    "### Step 6: Conditional Image Generation\n",
    "\n",
    "So far, we've trained an unconditional model that can generate random digits. But what if we want to control which digit gets generated?\n",
    "\n",
    "The idea is the same as we saw with the spiral/circle example: we add a label (the digit class, 0-9) as an additional input to the network. The network learns to associate each label with the corresponding digit's appearance.\n",
    "\n",
    "To implement this, we:\n",
    "1. **Modify the U-Net** to accept a label embedding alongside the time embedding\n",
    "2. **Modify the training** to pass the true digit labels\n",
    "3. **Modify the sampling** to specify which digit we want to generate\n",
    "\n",
    "This same principle extends to more complex scenarios: you could condition on text descriptions (like \"a red car\"), class labels (like \"cat\" or \"dog\"), or any other metadata. The network learns to associate these conditions with different modes of the data distribution.\n",
    "\n",
    "Here's the modified UNet module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eedce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Time Embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalEmbedding(32),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Add a label embedding layer\n",
    "        self.label_embedding = nn.Embedding(10, 32)\n",
    "        \n",
    "        # 1. Down Path\n",
    "        self.down1 = Block(1, 32, 32)\n",
    "        self.down2 = Block(32, 64, 32)\n",
    "        self.down3 = Block(64, 128, 32)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 2. Bottleneck\n",
    "        self.bottleneck = Block(128, 256, 32)\n",
    "        \n",
    "        # 3. Up Path\n",
    "        # We separate the Upsampling (ConvTrans) from the Block (Processing)\n",
    "        \n",
    "        # Up 1: 4x4 -> 8x8\n",
    "        self.up_trans1 = nn.ConvTranspose2d(256, 256, 4, 2, 1) \n",
    "        self.up1 = Block(256 + 128, 128, 32) # In: Bottle + Skip(x3)\n",
    "        \n",
    "        # Up 2: 8x8 -> 16x16\n",
    "        self.up_trans2 = nn.ConvTranspose2d(128, 128, 4, 2, 1)\n",
    "        self.up2 = Block(128 + 64, 64, 32)   # In: Prev + Skip(x2)\n",
    "        \n",
    "        # Up 3: 16x16 -> 32x32\n",
    "        self.up_trans3 = nn.ConvTranspose2d(64, 64, 4, 2, 1)\n",
    "        self.up3 = Block(64 + 32, 32, 32)    # In: Prev + Skip(x1)\n",
    "        \n",
    "        # Final projection\n",
    "        self.final = nn.Conv2d(32, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t, label):\n",
    "        t = self.time_mlp(t)\n",
    "        label_emb = self.label_embedding(label)\n",
    "        \n",
    "        t = t + label_emb\n",
    "        \n",
    "        # --- Down Path ---\n",
    "        x1 = self.down1(x, t)        # (32, 32, 32)\n",
    "        x_p1 = self.pool(x1)         # (32, 16, 16)\n",
    "        \n",
    "        x2 = self.down2(x_p1, t)     # (64, 16, 16)\n",
    "        x_p2 = self.pool(x2)         # (64, 8, 8)\n",
    "        \n",
    "        x3 = self.down3(x_p2, t)     # (128, 8, 8)\n",
    "        x_p3 = self.pool(x3)         # (128, 4, 4)\n",
    "        \n",
    "        # --- Bottleneck ---\n",
    "        x = self.bottleneck(x_p3, t) # (256, 4, 4)\n",
    "        \n",
    "        # --- Up Path ---\n",
    "        \n",
    "        # Step 1: Upsample -> Concat -> Process\n",
    "        x = self.up_trans1(x)                          # (256, 8, 8)\n",
    "        x = self.up1(torch.cat((x, x3), dim=1), t)     # (128, 8, 8)\n",
    "        \n",
    "        # Step 2\n",
    "        x = self.up_trans2(x)                          # (128, 16, 16)\n",
    "        x = self.up2(torch.cat((x, x2), dim=1), t)     # (64, 16, 16)\n",
    "        \n",
    "        # Step 3\n",
    "        x = self.up_trans3(x)                          # (64, 32, 32)\n",
    "        x = self.up3(torch.cat((x, x1), dim=1), t)     # (32, 32, 32)\n",
    "        \n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30437d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup ---\n",
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "dataset, dataloader = load_mnist(image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24655944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TIMESTEPS = 300\n",
    "EPOCHS = 5 # Start small to verify it runs\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# --- 3. Model & Utils ---\n",
    "model = ConditionalUNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Recalculate alphas/betas for the image process\n",
    "betas = torch.linspace(1e-4, 0.02, TIMESTEPS).to(device)\n",
    "alphas = 1 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375777fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noisy_image(x_0, t):\n",
    "    # This now handles batch of images (B, C, H, W)\n",
    "    sqrt_alpha_bar = torch.sqrt(alpha_bars[t])[:, None, None, None]\n",
    "    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n",
    "    \n",
    "    epsilon = torch.randn_like(x_0)\n",
    "    x_t = sqrt_alpha_bar * x_0 + sqrt_one_minus_alpha_bar * epsilon\n",
    "    return x_t, epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363b395",
   "metadata": {},
   "source": [
    "Now let's train the conditional model. The training process is the same as before, except now we pass the true digit labels to the network so it can learn the association between labels and digit appearances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Training Loop ---\n",
    "print(\"Starting Training...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_losses = []\n",
    "    for step, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 1. Sample timesteps\n",
    "        t = torch.randint(0, TIMESTEPS, (BATCH_SIZE,), device=device).long()\n",
    "        \n",
    "        # 2. Add Noise\n",
    "        x_t, epsilon = get_noisy_image(images, t)\n",
    "        \n",
    "        # 3. Predict Noise (with labels)\n",
    "        pred_epsilon = model(x_t, t, labels)\n",
    "        \n",
    "        # 4. Optimize\n",
    "        loss = loss_fn(pred_epsilon, epsilon)\n",
    "        epoch_losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "    print(f\"Average Epoch {epoch} Loss: {sum(epoch_losses) / len(epoch_losses):.4f}\")\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdce354c",
   "metadata": {},
   "source": [
    "The conditional sampling function is similar to the unconditional version, but now we specify which digits we want to generate. In this example, we'll generate one of each digit (0-9) to demonstrate that the model has learned to control generation based on the label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6dc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_mnist_conditional(model, timesteps):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Start with pure noise (Batch, 1, 32, 32)\n",
    "    x = torch.randn(10, 1, IMAGE_SIZE, IMAGE_SIZE).to(device)\n",
    "    \n",
    "    # Labels 0 to 9\n",
    "    labels = torch.linspace(0, 9, 10, device=device).long()\n",
    "    print(labels)\n",
    "    \n",
    "    # 2. Loop backwards\n",
    "    for t in range(timesteps - 1, -1, -1):\n",
    "        # Create batch of timesteps\n",
    "        t_batch = torch.full((10,), t, device=device, dtype=torch.long)\n",
    "        \n",
    "        # Get Model Prediction\n",
    "        predicted_noise = model(x, t_batch, labels)\n",
    "        \n",
    "        # Get constants for this step\n",
    "        # We reshape them to (1, 1, 1, 1) so they broadcast over the images\n",
    "        alpha_t = alphas[t].view(1, 1, 1, 1)\n",
    "        alpha_bar_t = alpha_bars[t].view(1, 1, 1, 1)\n",
    "        beta_t = betas[t].view(1, 1, 1, 1)\n",
    "        sigma_t = torch.sqrt(beta_t)\n",
    "        \n",
    "        # Algorithm: x_{t-1} = (x_t - ... * pred_noise) / sqrt(alpha)\n",
    "        if t > 0:\n",
    "            z = torch.randn_like(x)\n",
    "        else:\n",
    "            z = torch.zeros_like(x)\n",
    "            \n",
    "        x = (1 / torch.sqrt(alpha_t)) * (x - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * predicted_noise) + sigma_t * z\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7674a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 300\n",
    "# --- Run the Sampling ---\n",
    "print(\"Sampling from the model...\")\n",
    "generated_images = sample_mnist_conditional(model, timesteps)\n",
    "\n",
    "# --- Visualization ---\n",
    "# Un-normalize from [-1, 1] back to [0, 1]\n",
    "generated_images = (generated_images + 1) / 2\n",
    "generated_images = generated_images.clamp(0, 1).cpu()\n",
    "\n",
    "# Plot as a grid\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(generated_images[i, 0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Generated Digits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab3c7a",
   "metadata": {},
   "source": [
    "We can appreaciate how images above may look rather \"ugly\". This both because I decided to train for just a few epochs, keeping the model fairly simple, but also because we can resort to more advanced techniques (like Classifier-Free Guidance, in the following notebook) to improve our conditional generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc9371",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "One of the first applications that made diffusion models popular was exactly image generation. As you might have noticed, achieving this in its most essential formulation has not been very different from the simple DDPM example we saw in the first notebook. The leap forward to text conditioned image generation would not be conceptually too far as well. Imagine to substitute discrete numeric labels with some text-image embedding model like CLIP and taking a huge dataset of images + captions, and this is all pretty much we would need to realize a basic Stable Diffusion like model!\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Scaling to Images**: We adapted the diffusion framework from simple 2D points to real images by using convolutional architectures\n",
    "- **U-Net Architecture**: The encoder-decoder structure with skip connections is ideal for image denoising, preserving both high-level structure and fine details\n",
    "- **Conditional Generation**: By conditioning on labels, we can control what the model generates, opening the door to guided generation\n",
    "\n",
    "**What Makes This Powerful:**\n",
    "The same mathematical framework that worked for 2D spirals scales also to images. The U-Net's ability to process spatial information makes it suitable for learning complex image distributions. By training on many noise levels simultaneously, the model learns a robust denoising process that can generate diverse, high-quality samples.\n",
    "\n",
    "**Next Steps:**\n",
    "This notebook covered image generation with diffusion models, but there's still more to explore:\n",
    "- **Classifier-Free Guidance**: A technique to improve conditional generation quality by training with both conditional and unconditional objectives simultaneously\n",
    "- **Flow Matching**: An alternative formulation that can be faster and more stable than traditional diffusion\n",
    "- **Latent Diffusion**: Working in a compressed latent space (like Stable Diffusion) to generate high-resolution images efficiently\n",
    "- **Gradient-Based Optimization**: Optimize the generation at test-time guided by a function that steers the generation towards a specific objective\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
