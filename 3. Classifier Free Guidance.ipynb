{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1adaa5e3",
   "metadata": {},
   "source": [
    "## Classifier-Free Guidance\n",
    "\n",
    "In the previous notebooks, we've seen how to build conditional diffusion models that can generate specific types of images based on labels (like generating a specific digit from MNIST). However, there's a common problem with this approach: **the model sometimes ignores or only partially follows the conditioning signal**.\n",
    "\n",
    "This happens because the model has a natural tendency to generate \"generic valid images\" rather than being strongly steered by the label. The model learns that generating any plausible image is acceptable, so it doesn't always prioritize the specific condition we provide.\n",
    "\n",
    "**Classifier-Free Guidance (CFG)** is a powerful technique that solves this problem by training the model to perform both conditional and unconditional generation simultaneously. During inference, we can then blend these two predictions to get stronger adherence to the condition while maintaining high-quality outputs.\n",
    "\n",
    "In this notebook, we'll:\n",
    "- Understand the intuition behind classifier-free guidance\n",
    "- Modify our conditional U-Net to support both conditional and unconditional generation\n",
    "- Train the model with label dropout to learn both modes\n",
    "- Implement the guidance mechanism during sampling\n",
    "- Explore how different guidance scales affect generation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86579e1",
   "metadata": {},
   "source": [
    "### The Core Idea\n",
    "\n",
    "The key insight of classifier-free guidance is to train a single model that can operate in two modes:\n",
    "1. **Conditional mode**: $\\epsilon_\\theta(x_t, t, y)$ - predicts noise given the noisy image, timestep, and condition (label)\n",
    "2. **Unconditional mode**: $\\epsilon_\\theta(x_t, t)$ - predicts noise given only the noisy image and timestep\n",
    "\n",
    "During training, we randomly drop the condition (replace it with a special \"null\" token) so the model learns both behaviors. At inference time, we compute both predictions and combine them using a guidance scale $w$:\n",
    "\n",
    "$$\\epsilon_{final} = \\epsilon_\\theta(x_t, t) + w \\cdot (\\epsilon_\\theta(x_t, t, y) - \\epsilon_\\theta(x_t, t))$$\n",
    "\n",
    "This can be rewritten as:\n",
    "$$\\epsilon_{final} = (1-w) \\cdot \\epsilon_\\theta(x_t, t) + w \\cdot \\epsilon_\\theta(x_t, t, y)$$\n",
    "\n",
    "**Intuition**: The difference $(\\epsilon_\\theta(x_t, t, y) - \\epsilon_\\theta(x_t, t))$ represents the \"direction\" that the condition pushes the generation. By scaling this difference and adding it to the unconditional prediction, we amplify the effect of the condition.\n",
    "\n",
    "**Guidance Scale Behavior**:\n",
    "- $w = 0$: Pure unconditional generation (ignores the label completely)\n",
    "- $w = 1$: Standard conditional generation (no guidance amplification)\n",
    "- $w > 1$: Stronger adherence to the condition (higher values = stronger guidance, but can sometimes reduce diversity)\n",
    "\n",
    "The beauty of this approach is that we don't need a separate classifier or any additional models, as everything is learned within a single network.\n",
    "\n",
    "Reference paper: [_Classifier-Free Diffusion Guidance_](https://arxiv.org/abs/2006.11239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d94c258",
   "metadata": {},
   "source": [
    "### Step 1: Imports and Setup\n",
    "\n",
    "We'll reuse the same libraries and utilities from our previous image generation notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_t(x_0, t, alpha_bars):\n",
    "    epsilon = torch.randn_like(x_0)\n",
    "    x_t = torch.sqrt(alpha_bars[t]) * x_0 + torch.sqrt(1-alpha_bars[t]) * epsilon\n",
    "    \n",
    "    return x_t, epsilon\n",
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, t):\n",
    "            device = t.device\n",
    "            emb = torch.zeros(t.shape[0], self.embedding_dim, device=device)\n",
    "            \n",
    "            for i in range(self.embedding_dim // 2):\n",
    "                # We ensure the constant is on the correct device too\n",
    "                const = torch.tensor(10000.0, device=device) \n",
    "                omega_i = torch.exp(-(2*i/self.embedding_dim) * torch.log(const))\n",
    "                \n",
    "                emb[:, 2*i] = torch.sin(omega_i * t)\n",
    "                emb[:, 2*i+1] = torch.cos(omega_i * t)\n",
    "            \n",
    "            return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6504332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. First Convolution: Change channels (e.g. 1 -> 32)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 2. Time Projection: Map time to match 'out_ch'\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        \n",
    "        # 3. Second Convolution: Refine features\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # First Conv\n",
    "        h = self.conv1(x)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu(h)\n",
    "        \n",
    "        # Add Time Embedding\n",
    "        # (Batch, Time_Dim) -> (Batch, Out_Ch) -> (Batch, Out_Ch, 1, 1)\n",
    "        time_emb = self.time_mlp(t)\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2] # Broadcast to 4D\n",
    "        h = h + time_emb\n",
    "        \n",
    "        # Second Conv\n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.relu(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d691c6c",
   "metadata": {},
   "source": [
    "Now we define the full U-Net architecture. The key modification for classifier-free guidance is in the label embedding: we use **11 classes** instead of 10. The 11th class (index 10) represents the \"null\" or \"unconditional\" token that we'll use during training when we drop the label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff30e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFGUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Time Embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalEmbedding(32),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.label_embedding = nn.Embedding(11, 32)\n",
    "        \n",
    "        # 1. Down Path\n",
    "        self.down1 = Block(1, 32, 32)\n",
    "        self.down2 = Block(32, 64, 32)\n",
    "        self.down3 = Block(64, 128, 32)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 2. Bottleneck\n",
    "        self.bottleneck = Block(128, 256, 32)\n",
    "        \n",
    "        # 3. Up Path\n",
    "        # We separate the Upsampling (ConvTrans) from the Block (Processing)\n",
    "        \n",
    "        # Up 1: 4x4 -> 8x8\n",
    "        self.up_trans1 = nn.ConvTranspose2d(256, 256, 4, 2, 1) \n",
    "        self.up1 = Block(256 + 128, 128, 32) # In: Bottle + Skip(x3)\n",
    "        \n",
    "        # Up 2: 8x8 -> 16x16\n",
    "        self.up_trans2 = nn.ConvTranspose2d(128, 128, 4, 2, 1)\n",
    "        self.up2 = Block(128 + 64, 64, 32)   # In: Prev + Skip(x2)\n",
    "        \n",
    "        # Up 3: 16x16 -> 32x32\n",
    "        self.up_trans3 = nn.ConvTranspose2d(64, 64, 4, 2, 1)\n",
    "        self.up3 = Block(64 + 32, 32, 32)    # In: Prev + Skip(x1)\n",
    "        \n",
    "        # Final projection\n",
    "        self.final = nn.Conv2d(32, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t, label):\n",
    "        t = self.time_mlp(t)\n",
    "        label_emb = self.label_embedding(label)\n",
    "        \n",
    "        t = t + label_emb\n",
    "        \n",
    "        # --- Down Path ---\n",
    "        x1 = self.down1(x, t)        # (32, 32, 32)\n",
    "        x_p1 = self.pool(x1)         # (32, 16, 16)\n",
    "        \n",
    "        x2 = self.down2(x_p1, t)     # (64, 16, 16)\n",
    "        x_p2 = self.pool(x2)         # (64, 8, 8)\n",
    "        \n",
    "        x3 = self.down3(x_p2, t)     # (128, 8, 8)\n",
    "        x_p3 = self.pool(x3)         # (128, 4, 4)\n",
    "        \n",
    "        # --- Bottleneck ---\n",
    "        x = self.bottleneck(x_p3, t) # (256, 4, 4)\n",
    "        \n",
    "        # --- Up Path ---\n",
    "        \n",
    "        # Step 1: Upsample -> Concat -> Process\n",
    "        x = self.up_trans1(x)                          # (256, 8, 8)\n",
    "        x = self.up1(torch.cat((x, x3), dim=1), t)     # (128, 8, 8)\n",
    "        \n",
    "        # Step 2\n",
    "        x = self.up_trans2(x)                          # (128, 16, 16)\n",
    "        x = self.up2(torch.cat((x, x2), dim=1), t)     # (64, 16, 16)\n",
    "        \n",
    "        # Step 3\n",
    "        x = self.up_trans3(x)                          # (64, 32, 32)\n",
    "        x = self.up3(torch.cat((x, x1), dim=1), t)     # (32, 32, 32)\n",
    "        \n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76072768",
   "metadata": {},
   "source": [
    "We'll use the same MNIST dataset and data loading function as before. The dataset provides images and their corresponding digit labels (0-9), which we'll use for conditional generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53feb8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(image_size=32, batch_size=128, device='cpu'):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "    ])\n",
    "    dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "dataset, dataloader = load_mnist(image_size=IMAGE_SIZE, batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0be63",
   "metadata": {},
   "source": [
    "We'll also need our helper function to add noise to images during training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noisy_image(x_0, t):\n",
    "    # This now handles batch of images (B, C, H, W)\n",
    "    sqrt_alpha_bar = torch.sqrt(alpha_bars[t])[:, None, None, None]\n",
    "    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n",
    "    \n",
    "    epsilon = torch.randn_like(x_0)\n",
    "    x_t = sqrt_alpha_bar * x_0 + sqrt_one_minus_alpha_bar * epsilon\n",
    "    return x_t, epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4010a6",
   "metadata": {},
   "source": [
    "### Step 2: Training Setup\n",
    "\n",
    "Now we configure the training parameters. The key addition here is `LABEL_DROPOUT`, which controls the probability that we'll replace a real label with the null token (index 10) during training. This is what allows the model to learn both conditional and unconditional generation.\n",
    "\n",
    "A typical value for label dropout is around 0.1-0.2, meaning 10-20% of training samples will be unconditional. This ensures the model sees enough unconditional examples to learn that mode, while still primarily learning conditional generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 300\n",
    "EPOCHS = 5 \n",
    "\n",
    "LABEL_DROPOUT = 0.1 # Probability of dropping label conditioning\n",
    "\n",
    "# --- 3. Model & Utils ---\n",
    "model = CFGUnet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Recalculate alphas/betas for the image process\n",
    "betas = torch.linspace(1e-4, 0.02, TIMESTEPS).to(device)\n",
    "alphas = 1 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Training Loop ---\n",
    "print(\"Starting Training...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_losses = []\n",
    "    for step, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # apply label dropout\n",
    "        for i in range(labels.shape[0]):\n",
    "            if torch.rand(1) < LABEL_DROPOUT:\n",
    "                labels[i] = torch.tensor(10, device=device)\n",
    "        \n",
    "        # 1. Sample timesteps\n",
    "        t = torch.randint(0, TIMESTEPS, (BATCH_SIZE,), device=device).long()\n",
    "        \n",
    "        # 2. Add Noise\n",
    "        x_t, epsilon = get_noisy_image(images, t)\n",
    "        \n",
    "        # 3. Predict Noise\n",
    "        pred_epsilon = model(x_t, t, labels)\n",
    "        \n",
    "        # 4. Optimize\n",
    "        loss = loss_fn(pred_epsilon, epsilon)\n",
    "        epoch_losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "    print(f\"Average Epoch {epoch} Loss: {sum(epoch_losses) / len(epoch_losses):.4f}\")\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d567715",
   "metadata": {},
   "source": [
    "### Step 3: Sampling with Classifier-Free Guidance\n",
    "\n",
    "Now comes the exciting part: using classifier-free guidance during generation! The sampling function works as follows:\n",
    "\n",
    "1. **Start with pure noise** for the images we want to generate\n",
    "2. **For each timestep**, we need to compute both conditional and unconditional predictions:\n",
    "   - We duplicate the noisy images and timesteps\n",
    "   - We create two sets of labels: the real labels (0-9) and null labels (all 10s)\n",
    "   - We pass everything through the model in one batch to get both predictions\n",
    "3. **Apply the guidance formula**: We combine the predictions using the guidance scale\n",
    "4. **Update the images** using the combined prediction, just like in standard reverse diffusion\n",
    "\n",
    "The key insight is that we can efficiently compute both predictions in a single forward pass by batching them together. This makes classifier-free guidance computationally efficient, as we only need one model evaluation per timestep, not two separate ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec09a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_mnist(model):\n",
    "    model.eval()\n",
    "    n_samples = 10 # We want exactly 10 digits (0-9)\n",
    "    \n",
    "    # 1. Start with pure noise for 10 images\n",
    "    x = torch.randn(n_samples, 1, IMAGE_SIZE, IMAGE_SIZE).to(device)\n",
    "    \n",
    "    # Generate specific labels 0 through 9\n",
    "    labels = torch.arange(10, device=device).long()\n",
    "    \n",
    "    # Create the Null labels (all 10s)\n",
    "    null_labels = torch.full((n_samples,), 10, device=device).long()\n",
    "    \n",
    "    # Combine labels once (0..9, 10..10)\n",
    "    labels_in = torch.cat((labels, null_labels), dim=0)\n",
    "    \n",
    "    # 2. Loop backwards\n",
    "    for t in range(TIMESTEPS - 1, -1, -1):\n",
    "        # --- PREPARE INPUTS ---\n",
    "        # Double the noise x just for the model pass\n",
    "        x_in = torch.cat([x, x], dim=0)\n",
    "        \n",
    "        # Double the timestep t\n",
    "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        t_in = torch.cat((t_batch, t_batch), dim=0)\n",
    "        \n",
    "        # --- MODEL PASS ---\n",
    "        # We feed in batch of 20\n",
    "        predicted_noise = model(x_in, t_in, labels_in)\n",
    "        \n",
    "        # --- CLASSIFIER FREE GUIDANCE ---\n",
    "        # Split the output back into size 10\n",
    "        cond_pred = predicted_noise[:n_samples]\n",
    "        uncond_pred = predicted_noise[n_samples:]\n",
    "        \n",
    "        # Combine using the formula\n",
    "        # GUIDANCE_SCALE should be defined globally (e.g., 3.0 or 4.0)\n",
    "        epsilon = uncond_pred + GUIDANCE_SCALE * (cond_pred - uncond_pred)\n",
    "        \n",
    "        # --- UPDATE STEP ---\n",
    "        # Get constants reshaped to (1, 1, 1, 1) for broadcasting\n",
    "        alpha_t = alphas[t].view(1, 1, 1, 1)\n",
    "        alpha_bar_t = alpha_bars[t].view(1, 1, 1, 1)\n",
    "        beta_t = betas[t].view(1, 1, 1, 1)\n",
    "        sigma_t = torch.sqrt(beta_t)\n",
    "        \n",
    "        if t > 0:\n",
    "            z = torch.randn_like(x)\n",
    "        else:\n",
    "            z = torch.zeros_like(x)\n",
    "        \n",
    "        # Update the original x (size 10) using the combined epsilon (size 10)\n",
    "        x = (1 / torch.sqrt(alpha_t)) * (x - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * epsilon) + sigma_t * z\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df88a04",
   "metadata": {},
   "source": [
    "### Step 4: Generating Images with Guidance\n",
    "\n",
    "Let's test our trained model! We'll generate one of each digit (0-9) using a guidance scale of 2.0. This means we're amplifying the conditional signal by a factor of 2, which should result in stronger adherence to the specified labels compared to standard conditional generation ($w=1.0$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GUIDANCE_SCALE = 2.0\n",
    "# --- Run the Sampling ---\n",
    "print(\"Sampling from the model...\")\n",
    "generated_images = sample_mnist(model)\n",
    "\n",
    "# --- Visualization ---\n",
    "# Un-normalize from [-1, 1] back to [0, 1]\n",
    "generated_images = (generated_images + 1) / 2\n",
    "generated_images = generated_images.clamp(0, 1).cpu()\n",
    "\n",
    "# Plot as a grid\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(generated_images[i, 0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Generated Digits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eed016",
   "metadata": {},
   "source": [
    "Look at the generated digits above: we should be able to obtain higher quality generation with respect to the pure conditional version of our model with no guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd215fa6",
   "metadata": {},
   "source": [
    "### Exploring Different Guidance Scales\n",
    "\n",
    "One of the most interesting aspects of classifier-free guidance is how the guidance scale affects generation quality. Let's generate images at multiple guidance scales to see the effect:\n",
    "\n",
    "- **$w = 0.0$**: Pure unconditional generation (the model ignores labels completely)\n",
    "- **$w = 1.0$**: Standard conditional generation (no guidance amplification)\n",
    "- **$w = 2.0, 3.0, 4.0$**: Increasingly stronger guidance\n",
    "\n",
    "As we increase the guidance scale, we typically see:\n",
    "- **Stronger adherence** to the specified labels\n",
    "- **Potentially higher quality** samples (the model is more \"confident\" in following the condition)\n",
    "- **Reduced diversity** (the model becomes more deterministic)\n",
    "\n",
    "However, too high guidance scales can sometimes lead to artifacts or over-saturation. The optimal value depends on the task and model, but values between 2-4 are commonly used in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scales = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "all_generated_images = []\n",
    "\n",
    "for scale in guidance_scales:\n",
    "    GUIDANCE_SCALE = scale  # Assuming GUIDANCE_SCALE is a global variable used in sample_mnist\n",
    "    print(f\"Sampling from the model with GUIDANCE_SCALE={scale}...\")\n",
    "    images = sample_mnist(model)\n",
    "    images = (images + 1) / 2\n",
    "    images = images.clamp(0, 1).cpu()\n",
    "    all_generated_images.append(images)\n",
    "\n",
    "# Plot each guidance scale's samples in a column\n",
    "fig, axes = plt.subplots(len(guidance_scales), 10, figsize=(16, 5))\n",
    "for row, (scale, images) in enumerate(zip(guidance_scales, all_generated_images)):\n",
    "    for col in range(10):\n",
    "        ax = axes[row, col] if len(guidance_scales) > 1 else axes[col]\n",
    "        ax.imshow(images[col, 0], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    axes[row, 0].set_ylabel(f\"Scale {scale}\", fontsize=12)\n",
    "\n",
    "plt.suptitle(\"Generated Digits at Different Guidance Scales\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.02, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415400ff",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "Classifier-free guidance is one of the most important techniques in modern diffusion models. It was a key innovation that made models like DALL-E 2 and Stable Diffusion so effective at following text prompts and other conditions.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Dual Training**: The model learns both conditional and unconditional generation by randomly dropping conditions during training\n",
    "- **Guidance Formula**: At inference, we blend conditional and unconditional predictions to amplify the effect of the condition\n",
    "- **Single Model**: Unlike other guidance methods, CFG doesn't require a separate classifier, as everything is learned in one network\n",
    "- **Guidance Scale**: A hyperparameter that controls how strongly the model follows the condition (higher = stronger adherence, but potentially less diverse)\n",
    "\n",
    "**What Makes This Powerful:**\n",
    "The elegance of classifier-free guidance lies in its simplicity. By training the model to handle both conditional and unconditional cases, we get a natural way to control generation strength at inference time. The guidance mechanism effectively \"steers\" the generation in the direction specified by the condition, without requiring any additional models or complex training procedures.\n",
    "\n",
    "**Why It Works:**\n",
    "The difference $(\\epsilon_\\theta(x_t, t, y) - \\epsilon_\\theta(x_t, t))$ represents the \"signal\" that the condition provides. When we scale this difference and add it to the unconditional prediction, we're essentially asking: \"What would the unconditional model do, plus an amplified version of how the condition changes that prediction?\" This results in generation that follows the condition more strongly while maintaining the quality learned from unconditional training.\n",
    "\n",
    "**Next Steps:**\n",
    "This notebook covered classifier-free guidance for discrete labels, but the same principle extends to:\n",
    "- **Text conditioning**: Using text embeddings (like CLIP) instead of discrete labels\n",
    "- **Multi-conditioning**: Combining multiple conditions (e.g., text + style + class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
